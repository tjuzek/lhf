# How Learning from Human Feedback Influences the Lexical Choices of Large Language Models

This repository contains code and data for our paper: **"How Learning from Human Feedback Influences the Lexical Choices of Large Language Models"**


## Overview
Large Language Models (LLMs) are known to overuse certain words, such as *delve* and *intricate*. This project investigates whether Learning from Human Feedback (LHF) contributes to this phenomenon. We introduce a method for identifying potentially LHF-induced lexical preferences and critically, we conduct an experimental study to test our hypothesis, showing that LHF indeed influences lexical choices of LLMs.


## Contents
- **Paper:** The paper can be found in paper_vxxx.pdf; some of the procedures are explained in more detail in the paper, and if this is the case, pointers are given. Background, methodology, results, and conclusions are discussed in detail the paper.
- **Code:** These are the scripts used for our work. pipeline.md will talk you through the code step by step. 
- **Data:** The data analysed in the paper, most importantly the raw data of the experiment.


## Citation
We are trying to add an anonymous arxiv submission. Please check back again soon. 


## Licence 

Our own code and data are licensed under a **CC BY-SA** licence. External datasets and third-party resources retain their original licences.  


### Contact

The paper is under review, which is why this repo is anonymous. You can open an issue to interact on technical content. 
