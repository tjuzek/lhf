# How Learning from Human Feedback Influences the Lexical Choices of Large Language Models

This repository contains code and data for our paper: **"How Learning from Human Feedback Influences the Lexical Choices of Large Language Models"**


## Overview
Large Language Models (LLMs) are known to overuse certain words, such as *delve* and *intricate*. This project investigates whether Learning from Human Feedback (LHF) contributes to this phenomenon. We introduce a method for identifying potentially LHF-induced lexical preferences and critically, we conduct an experimental study to test our hypothesis. Our experimental findings are consistent with the hypothesis that Learning from Human Feedback influences the lexical choices of Large Language Models.


## Contents
- **Paper:** The paper can be found under [bias2025_v_2_0_0.pdf](https://github.com/tjuzek/lhf/blob/main/bias2025_v_2_0_0.pdf); some of the procedures are explained in more detail in the paper, and if this is the case, pointers are given. Background, methodology, results, and conclusions are discussed in detail the paper.
- **Code:** These are the scripts used for our work. pipeline.md will talk you through the code step by step. 
- **Data:** The data analysed in the paper, most importantly the raw data of the experiment.


## Citation
We are trying to add an anonymous arxiv submission. Please check back again soon. 


## Licence 

Our own code and data are licensed under a **CC BY-SA** licence. Some code was generated with the assistance of GitHub Copilot and is marked as such. External datasets and third-party resources retain their original licences.  


### Contact

Our websites have our contact details:

- [Tom Juzek](https://mll.fsu.edu/person/tom-juzek)  
- [Zina Ward](https://zinabward.com/)
